{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.special import logsumexp\nfrom typing import Tuple, Optional","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:53:15.774384Z","iopub.execute_input":"2025-07-30T09:53:15.774677Z","iopub.status.idle":"2025-07-30T09:53:21.390324Z","shell.execute_reply.started":"2025-07-30T09:53:15.774652Z","shell.execute_reply":"2025-07-30T09:53:21.389304Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class CurvedNeuralNetwork(nn.Module):\n    \"\"\"\n    Curved Neural Network implementation with deformation parameter gamma\n    Based on deformed exponential family distributions\n    \"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, \n                 gamma_prime: float = -0.5, beta: float = 1.0):\n        super().__init__()\n        \n        self.gamma_prime = gamma_prime  # Deformation parameter\n        self.beta = beta  # Inverse temperature\n        self.N = input_dim  # System size for scaling\n        \n        # Build network layers\n        layers = []\n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.Tanh()  # Using tanh as in mean-field equations\n            ])\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, output_dim))\n        \n        self.network = nn.Sequential(*layers)\n        \n        # Initialize weights using Hebbian-like rule for memory patterns\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize weights with small random values\"\"\"\n        for layer in self.network:\n            if isinstance(layer, nn.Linear):\n                nn.init.normal_(layer.weight, 0, 0.1)\n                nn.init.zeros_(layer.bias)\n    \n    def effective_temperature(self, energy: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute effective temperature β'(x) = β / (1 - γβE(x))\n        \"\"\"\n        gamma_scaled = self.gamma_prime / (self.N * self.beta)\n        return self.beta / (1 - gamma_scaled * energy)\n    \n    def energy_function(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute energy E(x) for the curved network\n        \"\"\"\n        # Simple quadratic energy for demonstration\n        energy = -0.5 * torch.sum(x * x, dim=-1)\n        return energy\n    \n    def deformed_activation(self, x: torch.Tensor, energy: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply deformed activation function with state-dependent temperature\n        \"\"\"\n        beta_eff = self.effective_temperature(energy)\n        \n        # Deformed exponential: [1 + γx]^(1/γ)_+\n        gamma_scaled = self.gamma_prime / (self.N * self.beta)\n        \n        if self.gamma_prime != 0:\n            deformed_input = 1 + gamma_scaled * x\n            # Ensure positivity\n            deformed_input = torch.clamp(deformed_input, min=1e-8)\n            output = torch.pow(deformed_input, 1/gamma_scaled)\n        else:\n            output = torch.exp(x)  # Standard exponential when γ → 0\n            \n        return output\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass with curved dynamics\n        \"\"\"\n        energy = self.energy_function(x)\n        beta_eff = self.effective_temperature(energy)\n        \n        # Standard forward pass\n        output = self.network(x)\n        \n        # Apply deformed activation if in explosive regime\n        if self.gamma_prime < 0:  # Explosive regime\n            output = self.deformed_activation(output, energy)\n        \n        return output, beta_eff","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:53:28.515939Z","iopub.execute_input":"2025-07-30T09:53:28.516798Z","iopub.status.idle":"2025-07-30T09:53:28.528757Z","shell.execute_reply.started":"2025-07-30T09:53:28.516750Z","shell.execute_reply":"2025-07-30T09:53:28.527690Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CurvedPACBayes:\n    \"\"\"\n    PAC-Bayes framework for curved neural networks\n    Derives generalization bounds accounting for curvature effects\n    \"\"\"\n    \n    def __init__(self, network: CurvedNeuralNetwork, m: int, delta: float = 0.05):\n        self.network = network\n        self.m = m  # Training set size\n        self.delta = delta  # Confidence parameter\n        \n    def kl_divergence_curved(self, posterior_params: dict, prior_params: dict) -> float:\n        \"\"\"\n        Compute KL divergence between curved distributions\n        KL(ρ||π) where ρ is posterior, π is prior\n        \"\"\"\n        # For Gaussian posterior and prior (simplified)\n        mu_post, sigma_post = posterior_params['mu'], posterior_params['sigma']\n        mu_prior, sigma_prior = prior_params['mu'], prior_params['sigma']\n        \n        # Standard KL for Gaussians, modified by curvature\n        kl_standard = 0.5 * (\n            torch.log(sigma_prior / sigma_post) + \n            (sigma_post**2 + (mu_post - mu_prior)**2) / sigma_prior**2 - 1\n        ).sum()\n        \n        # Curvature correction factor\n        gamma_correction = 1 + abs(self.network.gamma_prime) * kl_standard / self.network.N\n        \n        return kl_standard * gamma_correction\n    \n    def empirical_risk_curved(self, data_loader, loss_fn) -> float:\n        \"\"\"\n        Compute empirical risk accounting for curved dynamics\n        \"\"\"\n        total_loss = 0.0\n        total_samples = 0\n        \n        self.network.eval()\n        with torch.no_grad():\n            for x, y in data_loader:\n                output, beta_eff = self.network(x)\n                loss = loss_fn(output, y)\n                \n                # Weight loss by effective temperature (explosive dynamics)\n                if self.network.gamma_prime < 0:\n                    loss = loss * beta_eff.mean()  # Accelerated convergence\n                \n                total_loss += loss.item() * x.size(0)\n                total_samples += x.size(0)\n        \n        return total_loss / total_samples\n    \n    def pac_bayes_bound_mcallester(self, empirical_risk: float, kl_div: float) -> float:\n        \"\"\"\n        McAllester PAC-Bayes bound modified for curved networks\n        \"\"\"\n        # Standard McAllester bound\n        complexity_term = (kl_div + np.log(2 * np.sqrt(self.m) / self.delta)) / (2 * self.m - 1)\n        \n        # Curvature modification for explosive regime\n        if self.network.gamma_prime < 0:\n            # Explosive networks have enhanced generalization in certain regimes\n            explosive_factor = 1 - abs(self.network.gamma_prime) * 0.1  # Conservative estimate\n            complexity_term *= explosive_factor\n        \n        return empirical_risk + np.sqrt(complexity_term)\n    \n    def pac_bayes_bound_catoni(self, empirical_risk: float, kl_div: float, \n                               lambda_param: float = 1.0) -> float:\n        \"\"\"\n        Catoni PAC-Bayes bound for curved networks\n        \"\"\"\n        # Catoni's bound with λ parameter\n        kl_term = (kl_div + np.log(2 * np.sqrt(self.m) / self.delta)) / (lambda_param * self.m)\n        \n        # For curved networks, adjust λ based on effective temperature dynamics\n        if hasattr(self.network, 'last_beta_eff'):\n            avg_beta_eff = self.network.last_beta_eff.mean().item()\n            lambda_adjusted = lambda_param * avg_beta_eff\n        else:\n            lambda_adjusted = lambda_param\n            \n        # Catoni's exponential bound\n        if kl_term < 1:\n            bound = empirical_risk + kl_term + kl_term**2 / (2 * (1 - kl_term))\n        else:\n            bound = 1.0  # Vacuous bound\n            \n        return bound\n    \n    def stability_based_bound(self, data_loader, perturbation_scale: float = 0.01) -> float:\n        \"\"\"\n        Stability-based generalization bound for explosive networks\n        Leverages the self-regulating annealing property\n        \"\"\"\n        original_params = {}\n        for name, param in self.network.named_parameters():\n            original_params[name] = param.clone()\n        \n        # Compute original loss\n        original_loss = self.empirical_risk_curved(data_loader, nn.MSELoss())\n        \n        # Perturb parameters and compute loss\n        perturbed_losses = []\n        for _ in range(10):  # Multiple perturbations\n            for name, param in self.network.named_parameters():\n                noise = torch.randn_like(param) * perturbation_scale\n                param.data += noise\n            \n            perturbed_loss = self.empirical_risk_curved(data_loader, nn.MSELoss())\n            perturbed_losses.append(perturbed_loss)\n            \n            # Restore original parameters\n            for name, param in self.network.named_parameters():\n                param.data = original_params[name]\n        \n        # Stability measure\n        stability = np.std(perturbed_losses)\n        \n        # Bound based on stability (simplified)\n        stability_bound = original_loss + 2 * stability * np.sqrt(np.log(1/self.delta) / (2 * self.m))\n        \n        return stability_bound\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:53:42.211674Z","iopub.execute_input":"2025-07-30T09:53:42.212029Z","iopub.status.idle":"2025-07-30T09:53:42.229764Z","shell.execute_reply.started":"2025-07-30T09:53:42.212003Z","shell.execute_reply":"2025-07-30T09:53:42.228734Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CurvedNetworkTrainer:\n    \"\"\"\n    Training framework that optimizes both performance and PAC-Bayes bounds\n    \"\"\"\n    \n    def __init__(self, network: CurvedNeuralNetwork, pac_bayes: CurvedPACBayes):\n        self.network = network\n        self.pac_bayes = pac_bayes\n        self.training_losses = []\n        self.generalization_bounds = []\n    \n    def pac_bayes_objective(self, data_loader, loss_fn, lambda_reg: float = 0.1):\n        \"\"\"\n        Combined objective: empirical risk + PAC-Bayes regularization\n        \"\"\"\n        # Empirical risk\n        emp_risk = self.pac_bayes.empirical_risk_curved(data_loader, loss_fn)\n        \n        # Current parameters as posterior\n        posterior_params = {\n            'mu': torch.cat([p.flatten() for p in self.network.parameters()]),\n            'sigma': torch.ones_like(torch.cat([p.flatten() for p in self.network.parameters()])) * 0.1\n        }\n        \n        # Prior (zero mean, unit variance)\n        prior_params = {\n            'mu': torch.zeros_like(posterior_params['mu']),\n            'sigma': torch.ones_like(posterior_params['mu'])\n        }\n        \n        # KL divergence\n        kl_div = self.pac_bayes.kl_divergence_curved(posterior_params, prior_params)\n        \n        # PAC-Bayes bound\n        bound = self.pac_bayes.pac_bayes_bound_mcallester(emp_risk, kl_div.item())\n        \n        # Combined objective\n        objective = emp_risk + lambda_reg * bound\n        \n        return objective, emp_risk, bound\n    \n    def train_epoch(self, data_loader, optimizer, lambda_reg: float = 0.1):\n        \"\"\"\n        Train for one epoch with PAC-Bayes regularization\n        \"\"\"\n        self.network.train()\n        epoch_loss = 0.0\n        \n        for batch_idx, (x, y) in enumerate(data_loader):\n            optimizer.zero_grad()\n            \n            # Forward pass\n            output, beta_eff = self.network(x)\n            \n            # Store effective temperature for bound computation\n            self.network.last_beta_eff = beta_eff\n            \n            # PAC-Bayes objective\n            objective, emp_risk, bound = self.pac_bayes_objective(\n                data_loader, nn.MSELoss(), lambda_reg\n            )\n            \n            # Backward pass\n            if isinstance(objective, torch.Tensor):\n                objective.backward()\n            else:\n                # Convert to tensor if needed\n                loss_tensor = torch.tensor(objective, requires_grad=True)\n                loss_tensor.backward()\n            \n            optimizer.step()\n            \n            epoch_loss += objective if isinstance(objective, float) else objective.item()\n            \n            # Track explosive dynamics\n            if self.network.gamma_prime < 0 and batch_idx % 100 == 0:\n                print(f\"Batch {batch_idx}: β_eff = {beta_eff.mean().item():.4f}, \"\n                      f\"Bound = {bound:.4f}\")\n        \n        return epoch_loss / len(data_loader)\n    \n    def evaluate_generalization(self, train_loader, test_loader):\n        \"\"\"\n        Evaluate generalization performance and bounds\n        \"\"\"\n        # Training performance\n        train_risk = self.pac_bayes.empirical_risk_curved(train_loader, nn.MSELoss())\n        \n        # Test performance\n        test_risk = self.pac_bayes.empirical_risk_curved(test_loader, nn.MSELoss())\n        \n        # PAC-Bayes bounds\n        posterior_params = {\n            'mu': torch.cat([p.flatten() for p in self.network.parameters()]),\n            'sigma': torch.ones_like(torch.cat([p.flatten() for p in self.network.parameters()])) * 0.1\n        }\n        prior_params = {\n            'mu': torch.zeros_like(posterior_params['mu']),\n            'sigma': torch.ones_like(posterior_params['mu'])\n        }\n        \n        kl_div = self.pac_bayes.kl_divergence_curved(posterior_params, prior_params)\n        \n        mcallester_bound = self.pac_bayes.pac_bayes_bound_mcallester(train_risk, kl_div.item())\n        catoni_bound = self.pac_bayes.pac_bayes_bound_catoni(train_risk, kl_div.item())\n        stability_bound = self.pac_bayes.stability_based_bound(train_loader)\n        \n        results = {\n            'train_risk': train_risk,\n            'test_risk': test_risk,\n            'generalization_gap': test_risk - train_risk,\n            'mcallester_bound': mcallester_bound,\n            'catoni_bound': catoni_bound,\n            'stability_bound': stability_bound,\n            'kl_divergence': kl_div.item(),\n            'gamma_prime': self.network.gamma_prime\n        }\n        \n        return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:54:00.413433Z","iopub.execute_input":"2025-07-30T09:54:00.414460Z","iopub.status.idle":"2025-07-30T09:54:00.431746Z","shell.execute_reply.started":"2025-07-30T09:54:00.414381Z","shell.execute_reply":"2025-07-30T09:54:00.430394Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def run_curved_network_experiment():\n    \"\"\"\n    Demonstrate curved neural networks with explosive dynamics\n    \"\"\"\n    # Generate synthetic data\n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    n_samples = 1000\n    input_dim = 20\n    \n    X = torch.randn(n_samples, input_dim)\n    # Non-linear target with memory-like patterns\n    y = torch.sin(X.sum(dim=1, keepdim=True)) + 0.1 * torch.randn(n_samples, 1)\n    \n    # Split data\n    train_size = int(0.8 * n_samples)\n    train_X, test_X = X[:train_size], X[train_size:]\n    train_y, test_y = y[:train_size], y[train_size:]\n    \n    train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n    test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n    \n    # Test different gamma values\n    gamma_values = [0.0, -0.5, -1.0, -1.5]  # Explosive regime for negative values\n    results = {}\n    \n    for gamma in gamma_values:\n        print(f\"\\n=== Testing γ' = {gamma} ===\")\n        \n        # Create network\n        network = CurvedNeuralNetwork(\n            input_dim=input_dim,\n            hidden_dims=[64, 32],\n            output_dim=1,\n            gamma_prime=gamma,\n            beta=1.0\n        )\n        \n        # PAC-Bayes framework\n        pac_bayes = CurvedPACBayes(network, m=train_size)\n        trainer = CurvedNetworkTrainer(network, pac_bayes)\n        \n        # Training\n        optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n        \n        for epoch in range(50):\n            loss = trainer.train_epoch(train_loader, optimizer, lambda_reg=0.1)\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n        \n        # Evaluation\n        eval_results = trainer.evaluate_generalization(train_loader, test_loader)\n        results[gamma] = eval_results\n        \n        print(f\"Train Risk: {eval_results['train_risk']:.6f}\")\n        print(f\"Test Risk: {eval_results['test_risk']:.6f}\")\n        print(f\"Generalization Gap: {eval_results['generalization_gap']:.6f}\")\n        print(f\"McAllester Bound: {eval_results['mcallester_bound']:.6f}\")\n        print(f\"Catoni Bound: {eval_results['catoni_bound']:.6f}\")\n        print(f\"Stability Bound: {eval_results['stability_bound']:.6f}\")\n    \n    return results\n\n# Run experiment\nif __name__ == \"__main__\":\n    results = run_curved_network_experiment()\n    \n    # Analyze explosive behavior\n    print(\"\\n=== Analysis of Explosive Behavior ===\")\n    for gamma, result in results.items():\n        bound_tightness = result['mcallester_bound'] - result['generalization_gap']\n        print(f\"γ' = {gamma:4.1f}: Bound Tightness = {bound_tightness:.6f}, \"\n              f\"KL Div = {result['kl_divergence']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:54:29.580821Z","iopub.execute_input":"2025-07-30T09:54:29.581167Z","iopub.status.idle":"2025-07-30T09:56:17.698766Z","shell.execute_reply.started":"2025-07-30T09:54:29.581144Z","shell.execute_reply":"2025-07-30T09:56:17.697446Z"}},"outputs":[{"name":"stdout","text":"\n=== Testing γ' = 0.0 ===\nEpoch 0: Loss = 0.733523\nEpoch 10: Loss = 0.733523\nEpoch 20: Loss = 0.733523\nEpoch 30: Loss = 0.733523\nEpoch 40: Loss = 0.733523\nTrain Risk: 0.557970\nTest Risk: 0.500709\nGeneralization Gap: -0.057261\nMcAllester Bound: 1.755530\nCatoni Bound: 1.000000\nStability Bound: 0.558197\n\n=== Testing γ' = -0.5 ===\nBatch 0: β_eff = 1.3416, Bound = 11.0445\nEpoch 0: Loss = 3.260886\nBatch 0: β_eff = 1.3567, Bound = 11.0432\nBatch 0: β_eff = 1.3441, Bound = 11.0443\nBatch 0: β_eff = 1.3578, Bound = 11.0425\nBatch 0: β_eff = 1.3918, Bound = 11.0452\nBatch 0: β_eff = 1.3526, Bound = 11.0436\nBatch 0: β_eff = 1.3621, Bound = 11.0452\nBatch 0: β_eff = 1.3501, Bound = 11.0422\nBatch 0: β_eff = 1.3688, Bound = 11.0442\nBatch 0: β_eff = 1.3192, Bound = 11.0414\nBatch 0: β_eff = 1.3767, Bound = 11.0426\nEpoch 10: Loss = 3.260114\nBatch 0: β_eff = 1.3501, Bound = 11.0439\nBatch 0: β_eff = 1.3673, Bound = 11.0428\nBatch 0: β_eff = 1.3501, Bound = 11.0423\nBatch 0: β_eff = 1.3908, Bound = 11.0420\nBatch 0: β_eff = 1.3141, Bound = 11.0445\nBatch 0: β_eff = 1.3374, Bound = 11.0441\nBatch 0: β_eff = 1.3272, Bound = 11.0420\nBatch 0: β_eff = 1.3393, Bound = 11.0412\nBatch 0: β_eff = 1.4048, Bound = 11.0433\nBatch 0: β_eff = 1.3392, Bound = 11.0445\nEpoch 20: Loss = 3.260728\nBatch 0: β_eff = 1.3467, Bound = 11.0440\nBatch 0: β_eff = 1.3622, Bound = 11.0450\nBatch 0: β_eff = 1.3378, Bound = 11.0414\nBatch 0: β_eff = 1.3220, Bound = 11.0431\nBatch 0: β_eff = 1.3447, Bound = 11.0403\nBatch 0: β_eff = 1.4019, Bound = 11.0446\nBatch 0: β_eff = 1.3467, Bound = 11.0433\nBatch 0: β_eff = 1.3598, Bound = 11.0462\nBatch 0: β_eff = 1.3585, Bound = 11.0450\nBatch 0: β_eff = 1.3438, Bound = 11.0438\nEpoch 30: Loss = 3.261128\nBatch 0: β_eff = 1.3345, Bound = 11.0425\nBatch 0: β_eff = 1.3797, Bound = 11.0403\nBatch 0: β_eff = 1.3212, Bound = 11.0442\nBatch 0: β_eff = 1.3350, Bound = 11.0434\nBatch 0: β_eff = 1.4029, Bound = 11.0446\nBatch 0: β_eff = 1.3684, Bound = 11.0449\nBatch 0: β_eff = 1.3205, Bound = 11.0423\nBatch 0: β_eff = 1.3597, Bound = 11.0467\nBatch 0: β_eff = 1.3589, Bound = 11.0425\nBatch 0: β_eff = 1.3458, Bound = 11.0438\nEpoch 40: Loss = 3.260640\nBatch 0: β_eff = 1.3613, Bound = 11.0435\nBatch 0: β_eff = 1.3772, Bound = 11.0422\nBatch 0: β_eff = 1.4033, Bound = 11.0436\nBatch 0: β_eff = 1.3485, Bound = 11.0422\nBatch 0: β_eff = 1.3631, Bound = 11.0446\nBatch 0: β_eff = 1.3314, Bound = 11.0434\nBatch 0: β_eff = 1.3933, Bound = 11.0427\nBatch 0: β_eff = 1.3899, Bound = 11.0441\nBatch 0: β_eff = 1.3256, Bound = 11.0419\nTrain Risk: 2.156279\nTest Risk: 2.151609\nGeneralization Gap: -0.004670\nMcAllester Bound: 11.043481\nCatoni Bound: 1.000000\nStability Bound: 2.157165\n\n=== Testing γ' = -1.0 ===\nBatch 0: β_eff = 2.0911, Bound = 16.1878\nEpoch 0: Loss = 5.566741\nBatch 0: β_eff = 2.2276, Bound = 16.1587\nBatch 0: β_eff = 2.3607, Bound = 16.2045\nBatch 0: β_eff = 2.1796, Bound = 16.1666\nBatch 0: β_eff = 8.8745, Bound = 16.0942\nBatch 0: β_eff = 2.3299, Bound = 16.2503\nBatch 0: β_eff = 2.0207, Bound = 16.0623\nBatch 0: β_eff = 3.1580, Bound = 16.2449\nBatch 0: β_eff = 2.3777, Bound = 16.2398\nBatch 0: β_eff = 2.2451, Bound = 16.2269\nBatch 0: β_eff = 1.4925, Bound = 16.2818\nEpoch 10: Loss = 5.583863\nBatch 0: β_eff = 3.2885, Bound = 16.2209\nBatch 0: β_eff = 2.1799, Bound = 16.3066\nBatch 0: β_eff = 2.4618, Bound = 16.1214\nBatch 0: β_eff = 2.0048, Bound = 16.1934\nBatch 0: β_eff = 1.1112, Bound = 16.0956\nBatch 0: β_eff = 2.0372, Bound = 16.1162\nBatch 0: β_eff = 1.9128, Bound = 16.1860\nBatch 0: β_eff = 2.0670, Bound = 16.2887\nBatch 0: β_eff = 2.2867, Bound = 16.0950\nBatch 0: β_eff = 2.1154, Bound = 16.0017\nEpoch 20: Loss = 5.575009\nBatch 0: β_eff = 2.2670, Bound = 16.1679\nBatch 0: β_eff = 2.2457, Bound = 16.3311\nBatch 0: β_eff = 2.0974, Bound = 16.1146\nBatch 0: β_eff = 1.8916, Bound = 16.0007\nBatch 0: β_eff = 2.1050, Bound = 16.0699\nBatch 0: β_eff = 2.1052, Bound = 16.3409\nBatch 0: β_eff = 1.9861, Bound = 16.2690\nBatch 0: β_eff = 1.2849, Bound = 16.1842\nBatch 0: β_eff = 2.1037, Bound = 16.1712\nBatch 0: β_eff = 1.9152, Bound = 16.1720\nEpoch 30: Loss = 5.594346\nBatch 0: β_eff = 2.4193, Bound = 16.2623\nBatch 0: β_eff = 2.7713, Bound = 16.0520\nBatch 0: β_eff = 1.1522, Bound = 16.0872\nBatch 0: β_eff = 2.3484, Bound = 16.2739\nBatch 0: β_eff = 2.6196, Bound = 16.3081\nBatch 0: β_eff = 2.0327, Bound = 16.0113\nBatch 0: β_eff = 2.1180, Bound = 16.1758\nBatch 0: β_eff = 1.9647, Bound = 16.1960\nBatch 0: β_eff = 2.3434, Bound = 16.0624\nBatch 0: β_eff = 1.6428, Bound = 15.9788\nEpoch 40: Loss = 5.562711\nBatch 0: β_eff = 2.0601, Bound = 16.0150\nBatch 0: β_eff = 2.0077, Bound = 16.0232\nBatch 0: β_eff = 2.1466, Bound = 16.1328\nBatch 0: β_eff = 2.1695, Bound = 16.4021\nBatch 0: β_eff = 2.1361, Bound = 16.1259\nBatch 0: β_eff = 2.0794, Bound = 16.1930\nBatch 0: β_eff = 3.3320, Bound = 16.1973\nBatch 0: β_eff = 2.2197, Bound = 16.3192\nBatch 0: β_eff = 2.3282, Bound = 16.2235\nTrain Risk: 3.956430\nTest Risk: 3.153443\nGeneralization Gap: -0.802987\nMcAllester Bound: 16.134654\nCatoni Bound: 1.000000\nStability Bound: 3.945001\n\n=== Testing γ' = -1.5 ===\nBatch 0: β_eff = -0.6155, Bound = 28.7080\nEpoch 0: Loss = 21.450630\nBatch 0: β_eff = 8.2611, Bound = 30.9839\nBatch 0: β_eff = 7.1257, Bound = 32.1101\nBatch 0: β_eff = 6.5241, Bound = 32.7852\nBatch 0: β_eff = 1.1083, Bound = 29.8572\nBatch 0: β_eff = 4.3946, Bound = 29.1229\nBatch 0: β_eff = 3.6313, Bound = 33.6747\nBatch 0: β_eff = 3.1859, Bound = 29.4816\nBatch 0: β_eff = 3.1076, Bound = 34.4411\nBatch 0: β_eff = 1.1740, Bound = 32.2369\nBatch 0: β_eff = 10.2756, Bound = 31.5073\nEpoch 10: Loss = 21.071284\nBatch 0: β_eff = 203.5903, Bound = 37.1313\nBatch 0: β_eff = 1.7655, Bound = 32.6129\nBatch 0: β_eff = 6.4533, Bound = 30.9251\nBatch 0: β_eff = -1.5587, Bound = 31.6525\nBatch 0: β_eff = -0.0149, Bound = 28.8035\nBatch 0: β_eff = 17.3130, Bound = 32.9838\nBatch 0: β_eff = 3.9179, Bound = 28.9176\nBatch 0: β_eff = 5.3349, Bound = 30.6287\nBatch 0: β_eff = 1.4795, Bound = 29.6018\nBatch 0: β_eff = 0.3946, Bound = 33.2276\nEpoch 20: Loss = 20.930314\nBatch 0: β_eff = 0.6672, Bound = 33.1036\nBatch 0: β_eff = 2.4274, Bound = 30.4259\nBatch 0: β_eff = 2.1965, Bound = 33.6892\nBatch 0: β_eff = 198.0645, Bound = 33.8407\nBatch 0: β_eff = -1.4779, Bound = 33.6294\nBatch 0: β_eff = 2.6648, Bound = 34.1591\nBatch 0: β_eff = 12.2484, Bound = 28.0397\nBatch 0: β_eff = 1.9733, Bound = 30.4511\nBatch 0: β_eff = -8.9830, Bound = 32.0982\nBatch 0: β_eff = 5.7683, Bound = 32.4643\nEpoch 30: Loss = 21.175089\nBatch 0: β_eff = 5.2914, Bound = 29.1073\nBatch 0: β_eff = 3.1324, Bound = 29.7907\nBatch 0: β_eff = 3.5614, Bound = 36.1389\nBatch 0: β_eff = 2.1367, Bound = 31.9493\nBatch 0: β_eff = 5.4416, Bound = 35.4800\nBatch 0: β_eff = 7.0160, Bound = 35.3030\nBatch 0: β_eff = 6.5546, Bound = 32.0885\nBatch 0: β_eff = 2.6439, Bound = 34.1681\nBatch 0: β_eff = 2.4059, Bound = 32.1822\nBatch 0: β_eff = 2.1198, Bound = 30.5699\nEpoch 40: Loss = 21.135590\nBatch 0: β_eff = 13.9372, Bound = 32.3608\nBatch 0: β_eff = 3.9378, Bound = 28.7731\nBatch 0: β_eff = 197.8719, Bound = 36.4492\nBatch 0: β_eff = 5.2330, Bound = 29.0146\nBatch 0: β_eff = 1.9488, Bound = 35.3738\nBatch 0: β_eff = 2.9152, Bound = 32.4983\nBatch 0: β_eff = 198.1624, Bound = 32.8743\nBatch 0: β_eff = 196.1915, Bound = 36.4332\nBatch 0: β_eff = 2.0136, Bound = 36.3926\nTrain Risk: 16.273535\nTest Risk: 0.971652\nGeneralization Gap: -15.301883\nMcAllester Bound: 30.746238\nCatoni Bound: 1.000000\nStability Bound: 16.181500\n\n=== Analysis of Explosive Behavior ===\nγ' =  0.0: Bound Tightness = 1.812791, KL Div = 2286.174316\nγ' = -0.5: Bound Tightness = 11.048151, KL Div = 132932.750000\nγ' = -1.0: Bound Tightness = 16.937641, KL Div = 263488.875000\nγ' = -1.5: Bound Tightness = 46.048121, KL Div = 394022.562500\n","output_type":"stream"}],"execution_count":6}]}